{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497d1db-4763-4bba-af49-56f4ec1c4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2483e8fa-3ec5-4ade-baca-b14d2cac6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f8b1c8-cc67-47a7-b42b-81bd33351806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z's'q\\AppData\\Local\\Temp\\ipykernel_1568\\2964636124.py:4: FutureWarning: Support for Python 3.9 will be dropped in the next release (after its end-of-life on October 31, 2025). Please upgrade to Python 3.10 or newer.\n",
      "  from trl import SFTTrainer,SFTConfig\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer,SFTConfig\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    #print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f3ca9402-3062-40be-9d49-d452b5e9b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"datasets3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    geography_datasets = json.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "039fe8f0-26a6-4886-a0b1-ab07fdc2d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "geography_datasets[\"train\"] = [\n",
    "    {\"prompt\":item[\"question\"], \"completion\":item[\"sql\"]}\n",
    "    for item in geography_datasets[\"train\"]\n",
    "]\n",
    "\n",
    "geography_datasets[\"dev\"] = [\n",
    "    {\"prompt\":item[\"question\"], \"completion\":item[\"sql\"]}\n",
    "    for item in geography_datasets[\"dev\"]\n",
    "]\n",
    "\n",
    "geography_datasets[\"test\"] = [\n",
    "    {\"prompt\":item[\"question\"], \"completion\":item[\"sql\"]}\n",
    "    for item in geography_datasets[\"test\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fca6c-134b-4b51-9294-bae61b0ac77c",
   "metadata": {},
   "source": [
    "# Augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c02c9a0-15b1-46c7-8e03-fb655b792026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"newdatasets.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    geography_datasets = json.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c134bc30-7380-4d78-b208-149a22967924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "\n",
    "train_data =  Dataset.from_list(geography_datasets[\"train\"])\n",
    "dev_data =  Dataset.from_list(geography_datasets[\"dev\"])\n",
    "test_data =  Dataset.from_list(geography_datasets[\"test\"])\n",
    "dataset = DatasetDict({\"train\": train_data, \"dev\": dev_data, \"test\": test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7174bb-5e56-402f-a3fe-d61bb122eff9",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe4da1f-bb61-44d1-93c1-f95c938ec45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 169 new tokens.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check and add only the truly new tokens\n",
    "new_tokens_to_add = set(new_tokens) - set(tokenizer.vocab.keys())\n",
    "if new_tokens_to_add:\n",
    "    tokenizer.add_tokens(list(new_tokens_to_add))\n",
    "    print(f\"Added {len(new_tokens_to_add)} new tokens.\")\n",
    "else:\n",
    "    print(\"No new tokens to add.\")\n",
    "# Add a padding token to the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token # Use eos_token as pad_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#write your prompts and try to evaluate the model\n",
    "making_prompt = lambda x:x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b03808-c9ce-4c7c-b998-124cf6023a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5522117b126f440faa08e87f6902db0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a3ce9b3ecc4c789f43ec1a676d6c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mismatch between tokenized prompt and the start of tokenized prompt+completion. This may be due to unexpected tokenizer behavior, whitespace issues, or special token handling. Verify that the tokenizer is processing text consistently.\n",
      "Mismatch between tokenized prompt and the start of tokenized prompt+completion. This may be due to unexpected tokenizer behavior, whitespace issues, or special token handling. Verify that the tokenizer is processing text consistently.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b41c3afd824bdabe7d4f4c1b2cd3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06d9950e3fd43819fa69887a848f1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede569fead9a4e57b372c7a5f03202ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mismatch between tokenized prompt and the start of tokenized prompt+completion. This may be due to unexpected tokenizer behavior, whitespace issues, or special token handling. Verify that the tokenizer is processing text consistently.\n",
      "Mismatch between tokenized prompt and the start of tokenized prompt+completion. This may be due to unexpected tokenizer behavior, whitespace issues, or special token handling. Verify that the tokenizer is processing text consistently.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87472649139d4f0e896f0f1c67f3c5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 16:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.630800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.367400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.086400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('models/fine_tuned_model2\\\\tokenizer_config.json',\n",
       " 'models/fine_tuned_model2\\\\special_tokens_map.json',\n",
       " 'models/fine_tuned_model2\\\\chat_template.jinja',\n",
       " 'models/fine_tuned_model2\\\\vocab.json',\n",
       " 'models/fine_tuned_model2\\\\merges.txt',\n",
       " 'models/fine_tuned_model2\\\\added_tokens.json',\n",
       " 'models/fine_tuned_model2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try figure out how to set up the fine tuning config\n",
    "sft_config= SFTConfig(learning_rate=0.001,\n",
    "    torch_empty_cache_steps=3,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay = 0.001,\n",
    "    logging_steps=20,\n",
    "    completion_only_loss=True\n",
    "    )\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "# Create the SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    args=sft_config,\n",
    "    processing_class = tokenizer # Pass the tokenizer\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model('models/fine_tuned_model2')\n",
    "tokenizer.save_pretrained('models/fine_tuned_model2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88212f20-6b8d-4406-8af7-3887c72ee232",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c569ceac-4619-433f-82e9-407dc96b38b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: SELECT CITYalias0 = \"\" ; largestSELECT CITYalias0 = \" \" \" \" \"\" = \"\" > > ; outsideSELECT 1 instantaneous CITYalias0. \"\" > > > > > ; highestSELECT 1 reversible = \"\" ; supposed combined CITYalias0 = \"\" \"\" ; surroundingSELECT BORDER_INFO = \" \" \" \"\" CO = \" \" \" \" \"\" longerSELECT island\" > CITYalias0 = \" \" \"\" > (CITYalias1 = \"\" > ;}ushima = \"\" ;______FI = \" \"\" ;. CITYalias0 = \"\" = \"\" OR = \"\n"
     ]
    }
   ],
   "source": [
    "# Prepare prompt\n",
    "prompt = \"what is the biggest city of wyoming\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.98,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode and print only the new response\n",
    "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "print(\"Assistant:\", response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e802e-a57f-41f3-a864-c20f48407d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.8, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf9c09-1cdc-4036-aea0-608708f4b7ff",
   "metadata": {},
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e431424-0eab-4aa6-b849-f627c41caa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = ['BORDER_INFO','STATE_NAME','BORDER','CITY','CITY_NAME','STATE_NAME','POPULATION','COUNTRY_NAME','HIGHLOW','STATE_NAME','HIGHEST_POINT','HIGHEST_ELEVATION','LOWEST_POINT',\n",
    " 'LOWEST_ELEVATION','LAKE','LAKE_NAME','AREA','STATE_NAME','COUNTRY_NAME','MOUNTAIN','MOUNTAIN_NAME','MOUNTAIN_ALTITUDE','STATE_NAME','COUNTRY_NAME','RIVER','RIVER_NAME',\n",
    " 'LENGTH','TRAVERSE','COUNTRY_NAME','ROAD','ROAD_NAME','STATE_NAME','STATE','STATE_NAME','CAPITAL','POPULATION','AREA','COUNTRY_NAME','DENSITY','SELECT',\n",
    " 'CITYalias0','CITYNAME','FROM','AS','WHERE','MAX','CITYalias1','STATENAME','arizona','AND','texas','missouri','kansas','louisiana','california','rhode',\n",
    " 'island','new','mexico','nebraska','wyoming','michigan','minnesota','alabama','oregon','georgia','wisconsin','RIVERalias0','RIVERNAME','IN','STATEalias0',\n",
    " 'alaska','florida','ohio','maryland','maine','south','carolina','idaho','washington','north','dakota','massachusetts','mississippi','utah','hawaii',\n",
    " 'york','nevada','illinois','montana','iowa','hampshire','MIN','STATEalias1','virginia','pennsylvania','albany','LAKEalias0','LAKENAME','750','colorado',\n",
    " 'delaware','chattahoochee','potomac','red','HIGHLOWalias0','LOWESTELEVATION','HIGHESTPOINT','0','DISTINCT','RIVERalias1','COUNT','BORDERINFOalias0',\n",
    "'BORDERINFO','indiana','jersey','kentucky','arkansas','west','DERIVEDTABLEalias0','DERIVEDFIELDalias0','GROUP','BY','dallas','san','diego','antonio',\n",
    " 'austin','miami','plano','portland','rochester','salt','lake','city','kalamazoo','pittsburgh','flint','springfield','jose','scotts','valley',\n",
    " 'orleans','indianapolis','des','moines','boston','fort','wayne','houston','baton','rouge','denver','chicago','detroit','boulder','tucson','francisco',\n",
    " 'sacramento','seattle','montgomery','riverside','atlanta','HIGHESTELEVATION','HIGHLOWalias1','STATEalias2','STATEalias3','ORDER','DESC','LIMIT',\n",
    " '1','DERIVEDTABLEalias1','BORDERINFOalias1','HAVING','DERIVEDFIELDalias1','BORDERINFOalias2','NOT','mount','mckinley','guadalupe','peak',\n",
    " 'platte','rio','grande','150000','dc','minneapolis','erie','tempe','spokane','SUM','tennessee','vermont','oklahoma','durham','MOUNTAINalias0',\n",
    " 'MOUNTAINNAME','MOUNTAINALTITUDE','MOUNTAINalias1','RIVERalias2','LOWESTPOINT','COUNTRYNAME','usa','AVG','whitney','dover','columbus','salem',\n",
    " 'STATEalias4','STATEalias5','LEFT','OUTER','JOIN','ON','RIVERalias3','ALL','death','BORDERINFOalias3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29cbf51-43d7-4ec9-8943-18148e4c0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and add only the truly new tokens\n",
    "new_tokens_to_add = set(new_tokens) - set(tokenizer.vocab.keys())\n",
    "if new_tokens_to_add:\n",
    "    tokenizer.add_tokens(list(new_tokens_to_add))\n",
    "    print(f\"Added {len(new_tokens_to_add)} new tokens.\")\n",
    "else:\n",
    "    print(\"No new tokens to add.\")\n",
    "# Add a padding token to the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token # Use eos_token as pad_token\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
